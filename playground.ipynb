{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.970312  ,  0.06451527, -0.01109241, -0.1196076 , -0.21996865,\n",
       "        -0.32397598,  0.03425579, -0.09620602, -0.21570578, -0.13704151]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1 2 None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as session:\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(2)\n",
    "    c= tf.add(a,b)\n",
    "    p= tf.print(c)\n",
    "    c = tf.equal(c)\n",
    "    a_, b_, c_ = session.run([a,b,c])\n",
    "print(a_, b_, c_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0', tensor([[[[ 0.2026, -0.1416, -0.4792,  ..., -0.4328, -0.4474, -0.9484],\n",
      "          [ 0.2754, -0.5091, -0.7429,  ..., -0.9062, -0.4868, -0.7707],\n",
      "          [ 0.2036, -0.5745, -0.6625,  ..., -0.5499, -0.7129, -0.6586],\n",
      "          ...,\n",
      "          [ 0.7408,  0.1118, -0.0990,  ...,  0.3253,  0.4344,  0.0426],\n",
      "          [ 0.5034,  0.0354, -0.1463,  ...,  0.2960,  0.2485, -0.1045],\n",
      "          [ 0.1065, -0.1873, -0.2809,  ...,  0.1424, -0.2444, -0.2258]],\n",
      "\n",
      "         [[ 0.2340,  0.4896,  0.6568,  ...,  0.3670,  0.2520,  0.7608],\n",
      "          [-0.1051,  0.2342, -0.0032,  ..., -0.2923, -0.4759,  0.4552],\n",
      "          [-0.0556,  0.3019,  0.3993,  ..., -0.3940, -0.3351,  0.5760],\n",
      "          ...,\n",
      "          [-0.3989,  0.0774,  0.1446,  ..., -0.0164, -0.3375,  0.6226],\n",
      "          [-0.2259,  0.1098,  0.0954,  ..., -0.0390, -0.3937,  0.7688],\n",
      "          [-0.4894, -0.3702, -0.5208,  ..., -0.4016, -0.5206,  0.0541]],\n",
      "\n",
      "         [[-0.6215, -0.5989, -0.5461,  ..., -0.8714, -0.8918, -0.7630],\n",
      "          [-0.8586, -1.0617, -0.9666,  ..., -1.0406, -1.1174, -1.0309],\n",
      "          [-0.7950, -1.0045, -0.8850,  ..., -0.8646, -0.7279, -0.7146],\n",
      "          ...,\n",
      "          [-0.5707, -0.8411, -1.1031,  ..., -1.5817, -1.3594, -1.1963],\n",
      "          [-0.5464, -0.5326, -0.6870,  ..., -1.6295, -1.5699, -1.2559],\n",
      "          [-0.2636, -0.5020, -0.3950,  ..., -0.9168, -0.6340, -0.7278]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2393,  0.9562,  0.4366,  ...,  1.3165,  1.5165,  1.0716],\n",
      "          [ 0.7385,  1.2726,  0.9473,  ...,  1.8256,  1.7237,  1.3702],\n",
      "          [ 0.6142,  1.3124,  1.1870,  ...,  1.3679,  1.4142,  1.2903],\n",
      "          ...,\n",
      "          [ 0.1522,  0.4672,  0.6736,  ...,  1.2258,  1.6100,  1.1504],\n",
      "          [ 0.3982,  0.7171,  0.5820,  ...,  1.0979,  0.9638,  0.9125],\n",
      "          [ 0.1209,  0.0616,  0.0645,  ...,  0.0607,  0.1196,  0.1385]],\n",
      "\n",
      "         [[-0.1380,  0.0691, -0.1453,  ..., -0.0339, -0.0990,  0.5902],\n",
      "          [-0.5325, -0.0573,  0.1140,  ..., -0.1348,  0.0592,  0.5622],\n",
      "          [-0.4350, -0.2662,  0.0492,  ..., -0.1907, -0.0144,  0.6469],\n",
      "          ...,\n",
      "          [ 0.2286,  0.7792,  0.7344,  ...,  0.3848,  0.4169,  1.1278],\n",
      "          [ 0.2898,  0.8392,  0.8604,  ...,  0.5121,  0.4661,  1.3889],\n",
      "          [ 0.5222,  1.1300,  1.1410,  ...,  1.0258,  1.3307,  1.1693]],\n",
      "\n",
      "         [[-0.3234, -0.6324, -0.6329,  ..., -0.5867, -0.8890, -1.1491],\n",
      "          [-0.6656, -0.7916, -0.7021,  ..., -1.0005, -1.2395, -1.5479],\n",
      "          [-0.6387, -0.7194, -0.7272,  ..., -0.9437, -1.0465, -1.3881],\n",
      "          ...,\n",
      "          [-0.6624, -0.3812, -0.3207,  ..., -0.3492, -0.0378, -0.5989],\n",
      "          [-0.6075, -0.5081, -0.4763,  ..., -0.2699, -0.3945, -0.6895],\n",
      "          [-0.5114, -0.5428, -0.1638,  ..., -0.1559, -0.1036, -0.3379]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)), ('1', tensor([[[[ 0.2617, -0.0458, -0.0416,  ..., -0.3573, -0.7507, -0.4281],\n",
      "          [-0.0454, -0.1641, -0.3096,  ..., -0.5817, -0.4645, -0.3004],\n",
      "          [-0.2590, -0.3563, -0.4641,  ..., -0.7186, -0.5990, -0.2502],\n",
      "          ...,\n",
      "          [ 0.0058, -0.6288, -0.5130,  ..., -0.5971, -0.6719, -0.4611],\n",
      "          [-0.2648, -0.9212, -0.8693,  ..., -0.5591, -0.5750, -0.6088],\n",
      "          [ 0.0705, -0.2890, -0.4189,  ..., -0.1125, -0.0947,  0.0021]],\n",
      "\n",
      "         [[ 0.1331,  0.3436,  0.3584,  ..., -0.5244, -0.7010, -0.1242],\n",
      "          [ 0.2471,  0.2408,  0.5367,  ...,  0.0381,  0.0532,  0.1934],\n",
      "          [ 0.2299,  0.3321,  0.3668,  ..., -0.0866,  0.1090,  0.2218],\n",
      "          ...,\n",
      "          [ 0.2669,  0.4032,  0.2034,  ...,  0.2734,  0.4503,  0.5648],\n",
      "          [ 0.1911,  0.3345,  0.3284,  ...,  0.2180,  0.4930,  0.6388],\n",
      "          [ 0.4220,  0.4034,  0.4056,  ...,  0.3445,  0.5779,  0.6077]],\n",
      "\n",
      "         [[ 0.0383,  0.3221,  0.1652,  ..., -0.1738, -0.0611,  0.1697],\n",
      "          [-0.1889,  0.2495,  0.0832,  ...,  0.1043, -0.0056,  0.0028],\n",
      "          [-0.1224,  0.3502,  0.2654,  ...,  0.2354, -0.1012, -0.0728],\n",
      "          ...,\n",
      "          [-0.1174,  0.2012,  0.3021,  ...,  0.2499,  0.2872,  0.3568],\n",
      "          [-0.1410,  0.0665,  0.3033,  ...,  0.2777, -0.0825,  0.1279],\n",
      "          [ 0.0012,  0.0872,  0.1096,  ..., -0.0655, -0.0956, -0.0798]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2280, -0.0983, -0.0268,  ..., -0.0220,  0.3846,  0.0982],\n",
      "          [-1.0037, -0.7729, -1.0728,  ..., -0.8279, -0.7960, -0.7454],\n",
      "          [-0.8108, -0.8525, -1.0415,  ..., -0.6948, -0.7152, -0.6627],\n",
      "          ...,\n",
      "          [-0.2083,  0.0716, -0.1719,  ...,  0.0470, -0.0956,  0.0582],\n",
      "          [ 0.0241,  0.0781, -0.0533,  ...,  0.0989,  0.0495,  0.0103],\n",
      "          [ 0.0857, -0.1513, -0.0840,  ..., -0.0739,  0.0078, -0.2412]],\n",
      "\n",
      "         [[-0.6075, -0.3631, -0.3266,  ..., -0.6603, -0.4998, -0.4056],\n",
      "          [-0.3871, -0.3796, -0.3882,  ..., -0.9933, -0.8609, -0.7894],\n",
      "          [-0.2897, -0.3882, -0.3779,  ..., -0.7499, -0.8373, -0.7507],\n",
      "          ...,\n",
      "          [-0.2630, -0.1321,  0.0378,  ..., -0.4581, -0.1831, -0.1411],\n",
      "          [-0.2843, -0.1268, -0.0415,  ..., -0.1337,  0.1477,  0.0736],\n",
      "          [-0.0653,  0.0995,  0.0896,  ...,  0.1197,  0.3549, -0.0712]],\n",
      "\n",
      "         [[ 0.0140, -0.1481, -0.1823,  ..., -0.7432, -0.3034, -0.5508],\n",
      "          [ 0.5969,  0.2514,  0.2992,  ..., -0.2966,  0.0392, -0.3023],\n",
      "          [ 0.3891,  0.2251,  0.0741,  ..., -0.1485, -0.3780, -0.2859],\n",
      "          ...,\n",
      "          [-0.1272, -0.8059, -0.7344,  ..., -0.7650, -0.6667, -0.5830],\n",
      "          [-0.3805, -1.1164, -0.8143,  ..., -0.9234, -0.6293, -0.6935],\n",
      "          [ 0.0326, -0.5113, -0.3365,  ..., -0.3363, -0.2211, -0.2908]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)), ('2', tensor([[[[ 0.4209,  0.3997,  0.3540,  ...,  0.2293,  0.2221, -0.2174],\n",
      "          [ 0.5886,  1.1727,  0.9553,  ...,  0.5682,  0.5953,  0.2837],\n",
      "          [ 0.4937,  1.1768,  0.9342,  ...,  0.6023,  0.9507,  0.4718],\n",
      "          ...,\n",
      "          [ 0.4698,  0.5752,  0.5273,  ...,  0.6380,  0.6954, -0.0288],\n",
      "          [ 0.4687,  0.5177,  0.4593,  ...,  0.8735,  0.8602, -0.0680],\n",
      "          [ 0.2811,  0.2709,  0.2380,  ...,  0.7833,  0.5875,  0.1313]],\n",
      "\n",
      "         [[ 0.1716,  0.2919,  0.3112,  ...,  0.1886,  0.1524,  0.3984],\n",
      "          [ 0.2619,  0.8383,  0.9740,  ...,  0.5914,  0.5835,  0.5443],\n",
      "          [ 0.3778,  0.6353,  0.7834,  ...,  0.6899,  0.5813,  0.3752],\n",
      "          ...,\n",
      "          [ 0.1030,  0.3912,  0.4773,  ...,  0.8119,  0.7933,  0.3481],\n",
      "          [ 0.1210,  0.2641,  0.2873,  ...,  0.6478,  0.7276,  0.1435],\n",
      "          [ 0.3328,  0.2628,  0.2592,  ...,  0.1444,  0.2961,  0.2418]],\n",
      "\n",
      "         [[ 0.0052, -0.2311, -0.2313,  ..., -0.1201, -0.0079, -0.1596],\n",
      "          [ 0.0498,  0.3076,  0.1904,  ..., -0.0636, -0.1059, -0.3619],\n",
      "          [ 0.1647,  0.0573,  0.0908,  ..., -0.1619, -0.1804, -0.5341],\n",
      "          ...,\n",
      "          [ 0.0478, -0.0560,  0.0279,  ..., -0.2554, -0.1369, -0.4265],\n",
      "          [ 0.0312, -0.2436, -0.1265,  ..., -0.2752, -0.2722, -0.2965],\n",
      "          [ 0.0400,  0.1712,  0.0893,  ..., -0.2250, -0.1931, -0.1295]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1024,  0.5792,  0.5014,  ...,  0.5904,  0.5205,  0.6915],\n",
      "          [ 0.0164,  0.2368,  0.1281,  ...,  0.8913,  0.9144,  1.0834],\n",
      "          [ 0.0828,  0.1976, -0.0300,  ...,  0.9286,  0.9824,  1.0088],\n",
      "          ...,\n",
      "          [-0.1673, -0.0118,  0.2871,  ...,  1.1396,  1.0828,  0.8656],\n",
      "          [-0.0181,  0.2429,  0.4941,  ...,  0.9526,  0.9279,  0.8876],\n",
      "          [ 0.3397,  0.5428,  0.5732,  ...,  0.6836,  0.5415,  0.7164]],\n",
      "\n",
      "         [[-0.0026, -0.2804, -0.2845,  ..., -0.5651, -0.4626, -0.3757],\n",
      "          [ 0.1099,  0.0600, -0.0353,  ..., -0.3948, -0.2514, -0.0305],\n",
      "          [ 0.2512,  0.1515,  0.1737,  ..., -0.4090, -0.3344,  0.1522],\n",
      "          ...,\n",
      "          [ 0.0633, -0.1806, -0.1956,  ..., -0.5376, -0.4881,  0.0504],\n",
      "          [-0.0104, -0.2004, -0.1677,  ..., -0.4684, -0.5026, -0.1336],\n",
      "          [ 0.2658, -0.0391,  0.0623,  ...,  0.0467,  0.0832,  0.0811]],\n",
      "\n",
      "         [[ 0.0384,  0.0343,  0.2301,  ...,  0.5203,  0.5499, -0.2578],\n",
      "          [ 0.1097, -0.0961, -0.0436,  ...,  0.3951,  0.3019, -0.5193],\n",
      "          [ 0.1832, -0.1231, -0.1670,  ...,  0.3099,  0.1917, -0.5353],\n",
      "          ...,\n",
      "          [-0.0040,  0.1296,  0.1058,  ...,  0.4226,  0.5840, -0.1698],\n",
      "          [-0.1334, -0.1454, -0.0694,  ...,  0.4383,  0.7084,  0.0209],\n",
      "          [-0.2822, -0.1855, -0.0455,  ...,  0.1163,  0.2914,  0.0770]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)), ('3', tensor([[[[ 9.3073e-05,  8.9079e-02,  5.1356e-03,  ...,  9.2935e-03,\n",
      "           -5.1694e-02, -1.7035e-01],\n",
      "          [ 1.3517e-01, -3.0136e-01, -3.7774e-01,  ..., -2.9198e-01,\n",
      "           -2.2113e-01, -2.9379e-01],\n",
      "          [ 7.2002e-02, -1.5540e-01, -2.3520e-01,  ..., -7.7285e-02,\n",
      "           -8.6751e-02, -3.5931e-01],\n",
      "          ...,\n",
      "          [-8.0637e-02, -3.1476e-01, -4.6915e-01,  ..., -2.6443e-01,\n",
      "           -2.2001e-01, -3.0913e-01],\n",
      "          [-2.1831e-03, -2.1029e-01, -1.5625e-01,  ..., -3.9062e-01,\n",
      "           -2.2390e-01,  1.3861e-01],\n",
      "          [-2.0075e-02, -4.6491e-01, -5.4911e-01,  ..., -6.7860e-01,\n",
      "           -3.1172e-01, -5.7383e-02]],\n",
      "\n",
      "         [[-1.0671e-01, -8.8320e-03, -1.7377e-01,  ..., -3.2705e-01,\n",
      "           -4.5091e-01, -5.6181e-01],\n",
      "          [-2.0809e-01, -2.8765e-01, -4.6128e-01,  ..., -3.1991e-01,\n",
      "           -2.0038e-01, -1.6519e-01],\n",
      "          [-1.8302e-01, -2.7263e-01, -3.9365e-01,  ..., -3.0843e-01,\n",
      "           -1.3343e-01, -1.3196e-01],\n",
      "          ...,\n",
      "          [-1.2559e-01, -5.1466e-01, -7.1006e-01,  ..., -5.0988e-01,\n",
      "           -3.6690e-01, -3.2595e-01],\n",
      "          [-1.3054e-01, -2.7365e-01, -4.4317e-01,  ..., -1.7890e-01,\n",
      "           -3.9270e-01, -2.2210e-01],\n",
      "          [ 1.1446e-01,  7.7071e-02,  8.3069e-02,  ...,  4.9576e-01,\n",
      "            3.4234e-01, -5.3944e-02]],\n",
      "\n",
      "         [[ 1.9517e-01,  2.7971e-01,  1.2149e-01,  ..., -1.0259e-01,\n",
      "           -1.8585e-01, -2.5693e-01],\n",
      "          [ 3.5847e-02,  3.3237e-01,  3.6475e-01,  ...,  1.0133e-01,\n",
      "           -8.2257e-02, -2.6207e-02],\n",
      "          [ 6.6546e-02,  3.9946e-01,  4.0377e-01,  ...,  2.1945e-01,\n",
      "            1.3462e-01, -1.8663e-01],\n",
      "          ...,\n",
      "          [ 7.0842e-02,  4.2974e-01,  6.8068e-01,  ...,  3.4213e-01,\n",
      "            1.0402e-01,  1.9942e-01],\n",
      "          [-3.9585e-02,  4.9343e-01,  6.9335e-01,  ...,  3.6406e-01,\n",
      "            1.1519e-01,  1.3644e-01],\n",
      "          [ 2.8669e-01,  6.1369e-01,  5.7134e-01,  ...,  2.6838e-01,\n",
      "            3.6072e-01,  1.7912e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.6314e-01, -5.3092e-01, -5.0853e-01,  ..., -6.1602e-01,\n",
      "           -7.1224e-01, -2.4859e-01],\n",
      "          [-1.0001e+00, -1.0664e+00, -9.8989e-01,  ..., -9.0455e-01,\n",
      "           -8.7001e-01, -4.7772e-01],\n",
      "          [-8.7238e-01, -7.2997e-01, -6.9920e-01,  ..., -6.0144e-01,\n",
      "           -6.8718e-01, -2.7361e-01],\n",
      "          ...,\n",
      "          [-7.8665e-01, -6.4777e-01, -8.1467e-01,  ..., -8.7100e-01,\n",
      "           -1.2933e+00, -6.9196e-01],\n",
      "          [-7.6799e-01, -8.9870e-01, -7.9174e-01,  ..., -9.2927e-01,\n",
      "           -1.3539e+00, -7.0315e-01],\n",
      "          [-3.3567e-01, -3.0422e-01, -1.8221e-01,  ..., -4.8660e-01,\n",
      "           -5.3725e-01, -3.2340e-01]],\n",
      "\n",
      "         [[-4.3595e-01, -5.9731e-01, -4.9532e-01,  ..., -4.1841e-01,\n",
      "           -5.9870e-01, -2.9035e-01],\n",
      "          [-3.1575e-01, -3.9753e-01, -5.4934e-01,  ..., -5.2626e-01,\n",
      "           -5.0232e-01,  6.3326e-02],\n",
      "          [-2.1036e-01, -2.2002e-01, -2.9343e-01,  ..., -2.0716e-01,\n",
      "           -5.0448e-01,  1.9480e-01],\n",
      "          ...,\n",
      "          [-2.5781e-01, -3.2653e-01, -3.8094e-01,  ..., -3.8522e-01,\n",
      "           -5.7090e-01, -1.3249e-01],\n",
      "          [-5.7900e-01, -5.2314e-01, -5.0277e-01,  ..., -3.3690e-01,\n",
      "           -4.3106e-01, -2.3152e-01],\n",
      "          [-5.2025e-01, -1.9898e-01, -4.4387e-02,  ...,  2.3309e-01,\n",
      "            6.0740e-02,  4.4345e-01]],\n",
      "\n",
      "         [[ 1.6166e-01,  2.2320e-02, -1.4730e-01,  ..., -4.2880e-02,\n",
      "            3.5083e-02, -9.2031e-02],\n",
      "          [ 4.8832e-01,  2.0911e-01,  1.9203e-01,  ...,  3.6593e-01,\n",
      "            1.1728e-01, -4.1187e-01],\n",
      "          [ 4.2465e-01, -8.1819e-03, -1.3847e-01,  ...,  2.0154e-01,\n",
      "           -3.0755e-01, -5.8550e-01],\n",
      "          ...,\n",
      "          [ 4.4286e-01,  1.6806e-01,  2.0348e-01,  ...,  3.3819e-01,\n",
      "            7.5255e-02, -3.6007e-01],\n",
      "          [ 2.9709e-01,  2.0313e-01,  2.9024e-01,  ...,  3.1705e-01,\n",
      "            1.1602e-01, -4.9883e-01],\n",
      "          [ 5.6098e-01,  2.6096e-01,  2.9235e-01,  ...,  7.4896e-01,\n",
      "            4.4481e-01, -3.6055e-01]]]], grad_fn=<ConvolutionBackward0>)), ('pool', tensor([[[[ 9.3073e-05,  5.1356e-03, -3.1237e-02, -5.1694e-02],\n",
      "          [ 7.2002e-02, -2.3520e-01, -1.8947e-01, -8.6751e-02],\n",
      "          [-1.4675e-01, -3.5540e-01,  3.3292e-02,  9.0071e-02],\n",
      "          [-2.1831e-03, -1.5625e-01, -4.0094e-01, -2.2390e-01]],\n",
      "\n",
      "         [[-1.0671e-01, -1.7377e-01, -2.7978e-01, -4.5091e-01],\n",
      "          [-1.8302e-01, -3.9365e-01, -5.7905e-01, -1.3343e-01],\n",
      "          [-1.8137e-01, -5.5067e-01, -7.3431e-01, -3.3091e-01],\n",
      "          [-1.3054e-01, -4.4317e-01, -1.7525e-01, -3.9270e-01]],\n",
      "\n",
      "         [[ 1.9517e-01,  1.2149e-01,  1.8222e-01, -1.8585e-01],\n",
      "          [ 6.6546e-02,  4.0377e-01,  6.9052e-01,  1.3462e-01],\n",
      "          [ 1.9830e-01,  5.9072e-01,  5.6264e-01, -1.1150e-02],\n",
      "          [-3.9585e-02,  6.9335e-01,  5.6623e-01,  1.1519e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.6314e-01, -5.0853e-01, -7.0300e-01, -7.1224e-01],\n",
      "          [-8.7238e-01, -6.9920e-01, -8.0578e-01, -6.8718e-01],\n",
      "          [-9.2523e-01, -8.1689e-01, -7.6207e-01, -1.2037e+00],\n",
      "          [-7.6799e-01, -7.9174e-01, -8.9752e-01, -1.3539e+00]],\n",
      "\n",
      "         [[-4.3595e-01, -4.9532e-01, -5.8388e-01, -5.9870e-01],\n",
      "          [-2.1036e-01, -2.9343e-01, -4.4001e-01, -5.0448e-01],\n",
      "          [-2.1004e-01, -3.3603e-01, -3.7138e-01, -3.7163e-01],\n",
      "          [-5.7900e-01, -5.0277e-01, -4.7533e-01, -4.3106e-01]],\n",
      "\n",
      "         [[ 1.6166e-01, -1.4730e-01, -2.0135e-01,  3.5083e-02],\n",
      "          [ 4.2465e-01, -1.3847e-01,  9.7875e-02, -3.0755e-01],\n",
      "          [ 4.9371e-01,  1.1893e-01,  2.5397e-01,  6.2065e-02],\n",
      "          [ 2.9709e-01,  2.9024e-01,  6.7626e-02,  1.1602e-01]]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>))])\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "# print(model.out_channels)\n",
    "image= torch.randn((1,3,244,244))\n",
    "model.eval()\n",
    "result = model.backbone(image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': False, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('transform', GeneralizedRCNNTransform(\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "    Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      ")), ('backbone', BackboneWithFPN(\n",
      "  (body): IntermediateLayerGetter(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fpn): FeaturePyramidNetwork(\n",
      "    (inner_blocks): ModuleList(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (layer_blocks): ModuleList(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (extra_blocks): LastLevelMaxPool()\n",
      "  )\n",
      ")), ('rpn', RegionProposalNetwork(\n",
      "  (anchor_generator): AnchorGenerator()\n",
      "  (head): RPNHead(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")), ('roi_heads', RoIHeads(\n",
      "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "  (box_head): TwoMLPHead(\n",
      "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (box_predictor): FastRCNNPredictor(\n",
      "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      "  )\n",
      "))]), '_has_warned': False}\n"
     ]
    }
   ],
   "source": [
    "print(model.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = [torch.rand(3,244,244)]\n",
    "image, _ = model.transform(image)\n",
    "# print(image)\n",
    "features = model.backbone(image.tensors)\n",
    "proposals , proposals_L= model.rpn(image,features)\n",
    "box_features = model.roi_heads.box_roi_pool(features, proposals, image.image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 4])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([p for p in proposals]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 256, 7, 7])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4477],\n",
       "         [-0.1198]],\n",
       "\n",
       "        [[-0.2398],\n",
       "         [-0.3090]],\n",
       "\n",
       "        [[ 0.1378],\n",
       "         [-0.2618]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =torch.rand(3,2,5)\n",
    "model = torch.nn.Linear(5,1)\n",
    "model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "# from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from vild_pytorch.text_embedding import CLIP\n",
    "# import torchvision\n",
    "class a(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(a,self).__init__()\n",
    "        self.clip = CLIP()\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "                # frcnn_params = params[\"fcrnn_head\"]\n",
    "        layer_list = []\n",
    "        flatten_size = self.model.backbone.out_channels*self.model.roi_heads.box_roi_pool.output_size[0]**2\n",
    "        # input size: [B,N_proposal,256,7,7]\n",
    "        for i in range(4):\n",
    "            layer_list.append(nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        layer_list.append(nn.Flatten(start_dim=-3))\n",
    "        input_size = flatten_size\n",
    "        for i in range(2):\n",
    "            layer_list.append(nn.Linear(input_size,1024))\n",
    "            input_size = 1024\n",
    "        \n",
    "        layer_list.append(nn.BatchNorm1d(1024))\n",
    "\n",
    "        self.region = nn.Sequential( *layer_list )\n",
    "\n",
    "        self.vild_projection = nn.Sequential(\n",
    "            nn.Linear(input_size, 512)\n",
    "        )\n",
    "        self.model.rpn.training=False\n",
    "        self.background = nn.Parameter(torch.rand(1,512))\n",
    "    \n",
    "    def forward(self, images,categories, targets=None ):\n",
    "        \"\"\"\n",
    "            images (list[Tensor]): images to be processed\n",
    "            categories (list[String]): label list\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "            =============\n",
    "            this function is modified on torchvision.models.detection.generalized_rcnn\n",
    "        \"\"\"\n",
    "        categories = ['background'] + categories\n",
    "        categories = [{'name': item, 'id': idx+1,} for idx, item in enumerate(categories)]\n",
    "        result = []\n",
    "        original_image_sizes = [] # list[Tuple[int,int]]\n",
    "        for image in images:\n",
    "            shape = image.shape\n",
    "            # print(shape)\n",
    "            original_image_sizes.append([shape[-2], shape[-1]])\n",
    "            \n",
    "        images, targets = self.model.transform(images, targets)\n",
    "        features = self.model.backbone(images.tensors)\n",
    "        proposals, proposal_losses = self.model.rpn(images, features, targets)  # proposal:list[Tensor[1000, 4]]\n",
    "        # print(type(proposals))\n",
    "        # return proposals\n",
    "        # features = list(features.values())\n",
    "        # objectness, pred_bbox_deltas = self.model.rpn.head(features)\n",
    "        # anchors = self.model.rpn.anchor_generator(images, features)\n",
    "        # num_images = len(anchors)\n",
    "        # num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "        # num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "        # objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
    "        # # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
    "        # # note that we detach the deltas because Faster R-CNN do not backprop through\n",
    "        # # the proposals\n",
    "        # proposals = self.model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "        # proposals = proposals.view(num_images, -1, 4)\n",
    "        # boxes, scores = self.model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "\n",
    "        box_features = self.model.roi_heads.box_roi_pool(features, proposals, images.image_sizes) #[B* 1000, 256, 7, 7]\n",
    "        region_embedding = self.region(box_features) #[B*1000, 512]\n",
    "        region_embedding = self.vild_projection(region_embedding)\n",
    "        region_embedding = region_embedding.reshape(-1, len(proposals[0]), 512) #reshape it to [B, 1000, 512]\n",
    "\n",
    "        original_proposals = self.process_proposal(images, proposals, original_image_sizes ) #[B, 1000, 4]\n",
    "\n",
    "        text_embedding = self.clip(categories, embedding=\"text\")\n",
    "\n",
    "        #text_embedding = torch.cat([self.background, text_embedding], dim=0)\n",
    "        text_embedding = torch.tensor(text_embedding)\n",
    "        cls_logits = self.sim(region_embedding, text_embedding) #[B, 1000, Categories]\n",
    "        result = self.postprocess(cls_logits, original_proposals, original_image_sizes)\n",
    "        return result\n",
    "\n",
    "        # detections, detector_losses = self.model.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "        # detections = self.model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n",
    "\n",
    "    def process_proposal(self, images, proposals, original_image_sizes):\n",
    "        proposals = torch.stack(proposals, dim=0) # list[Tensor] -> tensor [B,1000, 4]\n",
    "        image_sizes = images.image_sizes\n",
    "        ratios = torch.tensor([\n",
    "            (torch.tensor(s_org[0], dtype=torch.float32, device=proposals.device)\n",
    "            / torch.tensor(s[0], dtype=torch.float32, device=proposals.device), \n",
    "            torch.tensor(s_org[1], dtype=torch.float32, device=proposals.device)\n",
    "            / torch.tensor(s[1], dtype=torch.float32, device=proposals.device)) \n",
    "            for s, s_org in zip(image_sizes, original_image_sizes)\n",
    "        ])\n",
    "        ratios_height, ratios_width = ratios.permute(1, 0) # transpose matrix\n",
    "        xmin, ymin, xmax, ymax = proposals.unbind(2)\n",
    "        xmin = xmin * ratios_width\n",
    "        ymin = ymin * ratios_height\n",
    "        xmax = xmax * ratios_width\n",
    "        ymax = ymax * ratios_height\n",
    "        return torch.stack((xmin,ymin,xmax,ymax), dim=2)\n",
    "\n",
    "    def sim(self, region_embedding, text_embedding):\n",
    "        NumBbox= region_embedding.shape[1] # Number of Bbox per image\n",
    "        NumCls = text_embedding.shape[0] # Number of Categories \n",
    "        ind = []\n",
    "        for i in range(NumBbox):\n",
    "            for j in range(NumCls):\n",
    "                ind.append(i+j*NumBbox)\n",
    "                # ind.append(i+NumBbox)\n",
    "        region_embeddingR = region_embedding.repeat((1,NumCls,1))[:,ind,:]  \n",
    "        text_embeddingR = text_embedding.repeat((NumBbox,1))\n",
    "        sim = nn.CosineSimilarity(dim=-1)\n",
    "        result = sim(region_embeddingR, text_embeddingR)\n",
    "        return result.reshape(-1,NumBbox,NumCls)\n",
    "\n",
    "    def postprocess(self, cls_logits, proposals, original_image_sizes):\n",
    "        \"\"\"\n",
    "        cls_logits: Tensor[B, 1000, Categories]\n",
    "        proposals: [B, 1000, 4]\n",
    "        original_image_sizes: [B, 4]\n",
    "        \"\"\"\n",
    "        scores = torch.nn.functional.softmax(cls_logits, -1)\n",
    "        device = cls_logits.device\n",
    "        num_class = cls_logits.shape[-1]\n",
    "        result=[]\n",
    "        proposals_x = proposals[... , 0::2] #[B, 1000, 2]\n",
    "        proposals_y = proposals[... , 1::2] #[B, 1000, 2]\n",
    "        p_list=[]\n",
    "        for p_x, p_y, img_s in zip(proposals_x, proposals_y, original_image_sizes):\n",
    "            height, width =img_s\n",
    "            p_x = p_x.clamp(min=0, max=width)  #[1000, 2]=[x1, x2]\n",
    "            p_y = p_y.clamp(min=0, max=height) #[1000, 2]=[y1, y2]\n",
    "            proposals_C = torch.stack([p_x, p_y], p_x.dim()) #[1000, 2, new_axis] -> p_C[0]=[[x1,y1],[x2,y2]]\n",
    "            proposal_C = proposals_C.reshape(-1, 4)\n",
    "            p_list.append(proposal_C)\n",
    "            # expend_ind = torch.arange\n",
    "        \n",
    "        # duplicate the class-agnostic proposals for each catergories\n",
    "        proposals = torch.stack(p_list, dim=0)\n",
    "        ind = torch.arange(proposals.shape[1]).reshape(-1, 1).repeat(1, num_class-1).reshape(-1)\n",
    "        proposals = proposals[:, ind, :] \n",
    "        all_boxes = []\n",
    "        all_scores = []\n",
    "        all_labels = []\n",
    "        for proposals_per_image, scores_per_image in zip(proposals, scores):\n",
    "            labels = torch.arange(num_class, device=device)\n",
    "            labels = labels.view(1, -1).expand_as(scores_per_image)\n",
    "\n",
    "            # proposals_per_image = proposals_per_image[:, 1:]\n",
    "            scores_per_image = scores_per_image[:, 1:]\n",
    "            labels = labels[:, 1:]\n",
    "            # batch everything, making every class prediction a separate instance\n",
    "            proposals_per_image = proposals_per_image.reshape(-1, 4)\n",
    "            scores_per_image = scores_per_image.reshape(-1)\n",
    "            labels = labels.reshape(-1)\n",
    "            # Threshold\n",
    "            inds = torch.where(scores_per_image > self.model.roi_heads.score_thresh)[0] \n",
    "            proposals_per_image, scores_per_image, labels = proposals_per_image[inds], scores_per_image[inds], labels[inds]\n",
    "\n",
    "            # Empty box\n",
    "            x_min, y_min, x_max, y_max = proposals_per_image[:, 0], proposals_per_image[:, 1], proposals_per_image[:, 2], proposals_per_image[:, 3]\n",
    "            area = (x_max-x_min)*(y_max-y_min)\n",
    "            inds2 = torch.where(area > 1e2)[0]\n",
    "            proposals_per_image, scores_per_image, labels = proposals_per_image[inds2], scores_per_image[inds2], labels[inds2]\n",
    "\n",
    "            # NMS by categories\n",
    "            proposals_per_image, scores_per_image, labels = self.NMS(proposals_per_image, scores_per_image, labels, 0.5)\n",
    "            \n",
    "            all_boxes.append(proposals_per_image)\n",
    "            all_scores.append(scores_per_image)\n",
    "            all_labels.append(labels)\n",
    "        \n",
    "        for i in range(len(all_boxes)):\n",
    "            result.append({\n",
    "                \"boxes\": all_boxes[i],\n",
    "                \"labels\": all_labels[i],\n",
    "                \"scores\": all_scores[i]\n",
    "            })\n",
    "        return result\n",
    "    \n",
    "    def NMS(self, proposals, scores, labels, box_nms_thresh):\n",
    "        labels_uniq = torch.unique(labels)\n",
    "        keep_p = []\n",
    "        keep_s = []\n",
    "        keep_l = []\n",
    "        for l in labels_uniq:\n",
    "            temp=[]\n",
    "            # select by class\n",
    "            ind_label = torch.where(labels == l)[0]\n",
    "            p_selected,s_selected=proposals[ind_label], scores[ind_label]\n",
    "            Xmin, Ymin, Xmax, Ymax = p_selected.unbind(1)\n",
    "            Area = (Xmax-Xmin)* (Ymax-Ymin)\n",
    "\n",
    "            # sort by socre\n",
    "            orders = s_selected.argsort(descending=True)\n",
    "            # p_selected, r_selected, l_selected = p_selected[orders], r_selected[orders], l_selected[orders]\n",
    "\n",
    "            while (len(orders) > 0):\n",
    "                i = orders[0]\n",
    "                temp.append(i)\n",
    "                xmin = torch.max(Xmin[i], Xmin[orders[1:]])\n",
    "                xmax = torch.min(Xmax[i], Xmax[orders[1:]])\n",
    "                ymin = torch.max(Ymin[i], Ymin[orders[1:]])\n",
    "                ymax = torch.min(Ymax[i], Ymax[orders[1:]])\n",
    "\n",
    "                w = torch.max(torch.tensor(0), xmax - xmin)\n",
    "                h = torch.max(torch.tensor(0), ymax - ymin)\n",
    "                inter = w*h\n",
    "\n",
    "                IoU = inter/(Area[i] + Area[orders[1:]] - inter)\n",
    "                ind_iou = torch.where(IoU <= box_nms_thresh)[0]\n",
    "                temp_iou = orders[ind_iou]\n",
    "                orders = temp_iou\n",
    "            keep_p.append(p_selected[temp])\n",
    "            keep_s.append(s_selected[temp]) \n",
    "            keep_l.append(torch.tensor([l]*ind_label.shape[0]))\n",
    "        \n",
    "        keep_p = torch.cat(keep_p, dim=0).reshape(-1, 4)\n",
    "        keep_s = torch.cat(keep_s, dim=0).reshape(-1)\n",
    "        keep_l = torch.cat(keep_l, dim=0).reshape(-1)\n",
    "        return keep_p, keep_s, keep_l\n",
    "\n",
    "        # return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "image = Image.open(\"3ppl.jpg\").convert(\"RGB\")\n",
    "transform1 = transforms.Compose([\n",
    "    # transforms.Resize((,244)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image = transform1(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Text Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 18.52it/s]\n"
     ]
    }
   ],
   "source": [
    "model = a()\n",
    "model.eval()\n",
    "# b = torch.rand((1,3,244,244))\n",
    "\n",
    "# result = model(image.unsqueeze(0), [\"luggage\", \"black luggage\", \"black object\"])\n",
    "# proposals, box_features = model([image, image])\n",
    "\n",
    "r = model(image.unsqueeze(0), [\"luggage\", \"fish\", \"mouse\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1717, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0][\"boxes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 4])\n"
     ]
    }
   ],
   "source": [
    "print(proposals[1].shape)\n",
    "# print(box_features.shape)\n",
    "# print(after_head.shape)\n",
    "# print(cls_logits.shape)\n",
    "# print(box_regression.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 256, 7, 7])\n",
      "torch.Size([1000, 1024])\n",
      "torch.Size([1000, 91])\n",
      "torch.Size([1000, 364])\n"
     ]
    }
   ],
   "source": [
    "print(box_features.shape)\n",
    "print(after_head.shape)\n",
    "print(cls_logits.shape)\n",
    "print(box_regression.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([424.1377,  36.1039, 710.6549, 615.7603])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposals[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([\n",
    "    [[1,2], [3,4], [1,1]],\n",
    "    [[2,1], [4,3], [2,2]],\n",
    "\n",
    "], dtype=torch.float32)\n",
    "\n",
    "b=torch.tensor([\n",
    "    [1,2], [3,4]\n",
    "], dtype=torch.float32)\n",
    "ind=[]\n",
    "L= a.shape[1]\n",
    "for i in range(L):\n",
    "    ind.append(i)\n",
    "    ind.append(i+L)\n",
    "a2 = a.repeat((1,2,1))[:,ind,:]\n",
    "b2 = b.repeat((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.tensor(np.arange(-10,5))\n",
    "print((a > 0).type(torch.int32).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iKYC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11 (default, Aug  6 2021, 08:56:27) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5963f472496cc48255043cf410147cee5da387b6f85165d99c9d723734b19a4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
